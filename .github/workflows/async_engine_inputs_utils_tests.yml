name: Async Engine, Inputs, Utils, Worker Test

on:
  push:
    branches:
      - main
    paths:
      - 'vllm/**'
      - 'tests/test_inputs.py'
      - 'tests/test_outputs.py'
      - 'tests/multimodal/**'
      - 'tests/utils_/**'
      - 'tests/standalone_tests/lazy_imports.py'
      - 'tests/transformers_utils/**'
  pull_request:
    branches:
      - main
    paths:
      - 'vllm/**'
      - 'tests/test_inputs.py'
      - 'tests/test_outputs.py'
      - 'tests/multimodal/**'
      - 'tests/utils_/**'
      - 'tests/standalone_tests/lazy_imports.py'
      - 'tests/transformers_utils/**'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}-${{ github.actor }}
  cancel-in-progress: true

jobs:
  amd-experimental-test:
    runs-on: [ self-hosted, amdexperimental ]
    defaults:
      run:
        shell: bash
    container:
      image: localhost:5000/flagos:vllm-cuda-amd64-test-20251216
      ports:
        - 80:80
      volumes:
        - /home/flagscale_cicd/docker/docker_build/docker_tokenizers:/home/gitlab-runner/tokenizers
        - /home/flagscale_cicd/docker/docker_build/docker_data:/home/gitlab-runner/data
      options: >-
        --gpus all
        --shm-size=500g
        --privileged
        --ipc=host
        --ulimit memlock=-1
        --ulimit stack=67108864
        --ulimit nofile=65535:65535
        --user root

    env:
      PYTHONUNBUFFERED: "1"
      PYTEST_ADDOPTS: "-v -s"

    steps:
      - name: Checkout Code
        uses: actions/checkout@v6.0.1
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          ssh-strict: true
          ssh-user: git
          persist-credentials: true
          clean: true
          sparse-checkout-cone-mode: true
          fetch-tags: false
          show-progress: true
          lfs: false
          submodules: false
          set-safe-directory: true

      - name: Pre-Check Environment
        timeout-minutes: 5
        run: |
          echo "===== System Info ====="
          uname -a
          python3 --version || python --version
          pip3 --version || pip --version

          echo "===== GPU Info ====="
          if command -v nvidia-smi &>/dev/null; then
            nvidia-smi
          else
            echo "⚠️ GPU check skipped. nvidia-smi is not available in the current environment."
          fi

          echo "===== Disk Space ====="
          df -h

          echo "===== Available memory ====="
          free -h

      - name: Install dependencies and build vLLM
        run: |
          pip install -r requirements/cuda.txt
          pip install -e . -vvv
        env:
          CMAKE_BUILD_PARALLEL_LEVEL: 4
    
      - name: Verify installation
        shell: bash
        run: |
          python3 -c "import vllm; print(f'vLLM version: {vllm.__version__}')"

      - name: GPU Usage Check / Verification
        run: |
          source .github/workflows/scripts/gpu_check.sh
          wait_for_gpu

      - name: Run lazy imports test
        run: |
          python3 tests/standalone_tests/lazy_imports.py
        continue-on-error: false

      - name: Run test_inputs.py
        run: pytest tests/test_inputs.py

      - name: Run test_outputs.py
        run: pytest tests/test_outputs.py

      - name: Run multimodal tests
        run: pytest tests/multimodal

      - name: Run utils_ tests
        run: pytest tests/utils_

      - name: Run transformers_utils tests
        run: pytest tests/transformers_utils
