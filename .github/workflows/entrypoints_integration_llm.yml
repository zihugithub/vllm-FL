name: Entrypoints Integration Test (LLM)

on:
  push:
    branches:
      - run_test251211
    paths:
      - 'vllm/**'
      - 'tests/entrypoints/llm/**'
      - 'tests/entrypoints/offline_mode/**'
  workflow_dispatch:  # Manual trigger

jobs:
  ubuntu-gpu-smoke-test:
    runs-on: self-hosted
    container:
      image: localhost:5000/flagos:vllm-cuda-amd64-test-20251216
      ports:
        - 80
      volumes:
        - /home/flagscale_cicd/docker/docker_build/docker_tokenizers:/home/gitlab-runner/tokenizers
        - /home/flagscale_cicd/docker/docker_build/docker_data:/home/gitlab-runner/data
      options: --gpus all --shm-size=500g --privileged --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --ulimit nofile=65535:65535 --user root

    steps:
      - name: Checkout Code
        uses: actions/checkout@v6.0.1
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          ssh-strict: true
          ssh-user: git
          persist-credentials: true
          clean: true
          sparse-checkout-cone-mode: true
          fetch-tags: false
          show-progress: true
          lfs: false
          submodules: false
          set-safe-directory: true
          retry-on-intr: fail=5,delay=180000

      - name: Entrypoints Integration Test (LLM)
        timeout-minutes: 40
        run: |
          cd tests
          export VLLM_WORKER_MULTIPROC_METHOD=spawn
          pytest -v -s entrypoints/llm --ignore=entrypoints/llm/test_generate.py --ignore=entrypoints/llm/test_collective_rpc.py
          pytest -v -s entrypoints/llm/test_generate.py # it needs a clean process
          pytest -v -s entrypoints/offline_mode # Needs to avoid interference with other tests
